{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Подгрузим все необходимые нам библиотеки"
      ],
      "metadata": {
        "id": "qCrIz5zCUyn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install pubchempy\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "import pubchempy as pbp\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup "
      ],
      "metadata": {
        "id": "dj6uLJRMKwtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для начала попробуем извлечь Compound id для всех соединений. Для этого воспользуемся библиотекой Pubchempy."
      ],
      "metadata": {
        "id": "dC0viigdU39W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Считываем изначальный датасет\n",
        "data = pd.read_csv(\"7.csv\")\n",
        "#Забиваем в переменные колонки по которым будем искать CID\n",
        "name = data['Name']\n",
        "norm = data['normalised_name']\n",
        "#Создаем пустой список в который будем вносить полученные значения\n",
        "cid = []\n",
        "#Запускаем цикл перебирающий все названия в столбце\n",
        "for i in range(5000):\n",
        "    #Поскольку в колонке name используются разные синонимы обозначающие одно и тоже стоит привести их к общему виду\n",
        "    response = requests.get(url=\"https://en.wikipedia.org/wiki/\" + name[i] )\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    #Если соединение есть в википедии то как правило в заголовке указано его тривиальное название, которое нам нужно\n",
        "    title = soup.find(id='firstHeading')\n",
        "    a = title.string\n",
        "    #Записываем полученное из википедии название вместо изначального (Если соединения нет в википедии response возвращает значение без изменений)\n",
        "    name.replace(to_replace = name[i], value = a)\n",
        "    #Находим все синонимы соединения в библиотеке pubchempy\n",
        "    synonyms = pbp.get_synonyms(name[i], 'name')\n",
        "    syn = (' '.join(str(ol) for ol in synonyms))\n",
        "    #Среди синонимов есть и CID номер. Ищем его используя регулярные выражения\n",
        "    ci = re.findall(\"'CID': \\d+\", syn)\n",
        "    st = (' '.join(str(j) for j in ci))\n",
        "    #Отделяем от найденой строки цифры и записываем их в переменную\n",
        "    vv = re.findall(r'\\d+', st)\n",
        "    #Проверим нашелся ли сид номер таким способом\n",
        "    if vv == []:\n",
        "        try:\n",
        "            #Если по столбцу name CID номер мы не получили попробуем его найти из второго столбца содержащего smiles\n",
        "            new = pbp.get_compounds(norm[i], 'smiles')  \n",
        "            b = ((' '.join(str(el) for el in new)))\n",
        "            #В результате мы получем \"Compound123\" из этого нам необходимы только цифровая часть\n",
        "            v = re.findall(r'\\d+', b)\n",
        "            fin = int(''.join(v))    \n",
        "            #Добавляем полученное цифровое значение в список\n",
        "            cid.append(fin)\n",
        "        except:\n",
        "            #Если не получилось извлечь CID записываем NaN\n",
        "            cid.append('NaN')\n",
        "    else:\n",
        "        fin = int(''.join(vv))\n",
        "        #Если значение изнально нашлось среди синонимов тривиального названия оно добавляется в список\n",
        "        cid.append(fin)"
      ],
      "metadata": {
        "id": "mzDzWduXK0gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Полученные значения запишем в файл txt поскольку excel может автоматически отформатировать полученные значения при запуске файла"
      ],
      "metadata": {
        "id": "JkhIfDruVopA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MyFile = open ('output.txt', 'w')\n",
        "for element in cid:\n",
        "    MyFile.write(str(element))\n",
        "    MyFile.write('\\n')\n",
        "MyFile.close()"
      ],
      "metadata": {
        "id": "Su8SbMk1Ll8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Восстановим все данные DOI"
      ],
      "metadata": {
        "id": "19stjNGWHNly"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f6GECSqz4hM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup \n",
        "!pip install crossref-commons\n",
        "import crossref_commons.retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Считываем файл содержащий исправленные DOI (в изначальном датасете некоторые DOI слиплись с названием журнала)\n",
        "DOI = pd.read_csv('DOI.csv', encoding='latin-1')\n",
        "#Удалим дубликаты для ускорения работы алгоритма\n",
        "DO = DOI.drop_duplicates()\n",
        "doi = DO['DOI']\n",
        "doi = doi.reset_index(drop = True)"
      ],
      "metadata": {
        "id": "67CiZxhR0FBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DO.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHIPJLby3-lS",
        "outputId": "6d517b7b-fbec-40ea-cfab-ae8e809f4ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2822, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "После удаления дубликатов осталось 2822 уникальных строк для которых мы будем искать дату публикации статьи, название журнала и название статьи."
      ],
      "metadata": {
        "id": "quktZtEYHpBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Создаем пустые списки в которые будем записывать полученные данные\n",
        "publish_date = []\n",
        "artical_title = []\n",
        "jornal_name = []\n",
        "#Пройдемся по всем уникальным DOI\n",
        "for i in range(2822):\n",
        "  try:\n",
        "    #С помощью библиотки извлечем данные публикации\n",
        "    tmp = crossref_commons.retrieval.get_publication_as_json(doi[i])\n",
        "    #В полученом словаре найдем дату выхода статьи\n",
        "    date = tmp['indexed']['date-time']\n",
        "    publ_date = date.split('T')[0]\n",
        "    #После отделения времени от даты записываем полученную дату в список\n",
        "    publish_date.append(publ_date)\n",
        "    #Таким же спосоюом найдем название статьи\n",
        "    titile = tmp['title']\n",
        "    tit = (' '.join(str(ol) for ol in titile))\n",
        "    #Записываем полученное значение \n",
        "    artical_title.append(tit)\n",
        "    #И наконец найдем название журнала\n",
        "    jornal= tmp['short-container-title']\n",
        "    jorn = (' '.join(str(ol) for ol in jornal))\n",
        "    #Его тоже записываем\n",
        "    jornal_name.append(jorn)\n",
        "  except:\n",
        "    #Если статья не найдена добавляем во все списки 'Article not found'\n",
        "    doi[i] = 'Article not found'\n",
        "    publish_date.append('Article not found')\n",
        "    artical_title.append('Article not found')\n",
        "    jornal_name.append('Article not found')\n",
        "#Создаем датафрейм из ролученных списков\n",
        "DO['DOI'] = doi\n",
        "DO['Date'] = publish_date\n",
        "DO['Journal'] = jornal_name\n",
        "DO['Title'] = artical_title\n",
        "#Проверим выходной датафрейм\n",
        "print(DO)"
      ],
      "metadata": {
        "id": "5ZaMYHkR2Vpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Полученные данные запишем в xlsx файл для дальнейшего использования"
      ],
      "metadata": {
        "id": "aIRjUeG4JpxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DO.to_excel('DOI_new.xlsx', index = False)"
      ],
      "metadata": {
        "id": "_RzL5Dvc6INz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pd.read_excel(\"7_new.xlsx\"))\n",
        "data = pd.DataFrame(pd.read_excel('DOI_new.xlsx'))"
      ],
      "metadata": {
        "id": "j07gW0ht0nAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "При помощи функции merge востанавливаваем убранные ранее дубликаты. Для это используем left.join а индексной колонкой делаем общую колонку 'DOI'"
      ],
      "metadata": {
        "id": "jGIma9AIbw-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat = df['DOI']\n",
        "fin = pd.merge(dat, data, how ='left', on = 'DOI')"
      ],
      "metadata": {
        "id": "kycQdIed08eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выгружаем полученный датасет и проверяем его"
      ],
      "metadata": {
        "id": "lFF8Pw72b9Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fin.to_excel('fin.xlsx', index = False)"
      ],
      "metadata": {
        "id": "IRFAD1CS1FiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поскольку в большинстве библиотек паристь удобнее по smiles найдем его"
      ],
      "metadata": {
        "id": "cwzO2kg8cFyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "!pip install pubchempy\n",
        "import pubchempy as pbp"
      ],
      "metadata": {
        "id": "OA16K-g9EeKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Считываем датасет\n",
        "data = pd.read_excel('7_whit_cid.xlsx')\n",
        "CID = data['CID']\n",
        "#Удаляем дубликаты для ускорения работы алгоритма\n",
        "cid = CID.drop_duplicates() \n",
        "ci = cid['CID'].tolist()"
      ],
      "metadata": {
        "id": "YLCqnwimEhDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#создадим пустой список в который будем записывать значения\n",
        "canonical_smiles = []\n",
        "for i in ci:\n",
        "  try:\n",
        "    #Находим smiles по CID которые ншли ранее\n",
        "    c = pbp.Compound.from_cid(i)\n",
        "    a = c.canonical_smiles\n",
        "    canonical_smiles.append(a)\n",
        "  except:\n",
        "    #В случае ошибки вместо значения записываем NaN\n",
        "    canonical_smiles.append('NaN')"
      ],
      "metadata": {
        "id": "78BR-0_bEkc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для правильного объединения изначального датасета и полученных smiles вставим колонку CID в датафрейм с дескрипторами"
      ],
      "metadata": {
        "id": "yb-X6LP-d27F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "canonical_smiles = canonical_smiles.insert(0, \"CID\", cid)"
      ],
      "metadata": {
        "id": "djcEP_b5dcaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "При помощи функции merge востанавливаваем убранные ранее дубликаты. Для это используем left.join а индексной колонкой делаем общую колонку 'CID'.\n",
        "Полученный датафрейм выгружаем и провеяем."
      ],
      "metadata": {
        "id": "T5P-DA6Id3fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smiles = pd.merge(CID, canonical_smiles, how ='left', on = 'CID')\n",
        "smiles.to_excel('smiles.xlsx', index = False)"
      ],
      "metadata": {
        "id": "HbLcWwvPEnqL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}