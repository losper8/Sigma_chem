<h1 align="center">Hi there, we are <a href="https://daniilshat.ru/" target="_blank">team SIGMA</a> </h1>
<h1 align="center"> <img src="https://camo.githubusercontent.com/8bf6f6d78abc81fcf9c49f10649423e73ea44bc248e83aaae8759d401c829a84/68747470733a2f2f70687973696373677572756b756c2e66696c65732e776f726470726573732e636f6d2f323031392f30322f6368617261637465722d312e676966" height="100"/></h1>
Перед нами стояло 3 задачи:

  1. В исходный датасет спарсить как минимум 1000 новых дескрипторов из трех различных API. И провести очистку полученного датасета.
  2. Визуализировать полученные данные 
  3. Провести кластеризацию данных, провести feature engineering и далее реализовать автоматический выбор фичей в контексте задачи машинного обучения.


Перед тем как начать парсить данные мы обработали изначальный датасет. Актуализировали информация о статьях и получили названия соединений в формате 'smiles' (см. файл Data_optimisation.IPYNB).

В ходе выполнения первой задачей было извлечено 38 дескрипторов из библиотеки PubChempy (см. файл pubchem_parser.IPYNB), 123 дескриптора из API RdKit (см. файл rdkit_parser.IPYNB) и 1800 дескрипторов из библиотки mordred (см. файл mordred_parser.IPYNB).
Все полученные дескрипторы мы добавили в общий датасет - 7_whith_cid.xlsx.


Визуализация
Для визуализации данных можно использовать интерактивный график с различными статистическими данными такими как среднее, стд, медиана, q1, q2, iqr. Так же выполняется тест Шапирио-Улика на нормальнось распределения. Строится график распределения частоты значений и бокс плот. На данных графиках можно увидеть распределение частот значений, что позволяет где в основном распределенны значения. Так же после класстерезации проведенной в следующем пункет можно посчиать колличество элемментов кажодго класстера. Показанно так же совместное распределние икслогп и тпса. Один параметр отвечкает за растворимость, другой описывает морфологию атома и несмотря на то, что прямой связи ними есть, прееидически встречаются зависимости, как можно заметить на скетерплот графике.
Посчитанно корреляционная матрица, убраны стобцы с корреляцией выше 0.9

Для того чтобы потобрать наиболее оптимальный алгоритм кластеризации для наших данных нами были протетированы ряд алгоритмов. Лучше всего с поставленной задачей справился алгоритм
GaussianMixture (см. файл Clustering.IPYNB). 

Далее во время Feature engineering (см. файл feature_creation.IPYNB) используя AutoFeatRegressor мы получили ряд новых фичей полученных путем различных математических преобразований изначальных фичей.

Мы решили посмотреть, какие фичи обладают наибольшей важностью (см. файл Feature_sel&imp.IPYNB). И с помощью ряда алгоритмов из которых лучщим оказался SequentialFeatureSelector, получили 5 наиболее важных фичей.

<h5 align="center">  <a href="https://daniilshat.ru/" target="_blank">Thanks for your attention!</a>  </h5>
<h1 align="center"> <img src="https://pixelbox.ru/wp-content/uploads/2021/08/ava-animation-cats-85.gif" height="100"/> </h1>
